{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some consideration before starting:\n",
    "-Edit cifar10.py file, setting the directory where CIFAR10 dataset is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from cifar10 import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "cifar10 = CIFAR10(batch_size=100, validation_proportion=0.1)\n",
    "\n",
    "SUMMARIES_DIR = './summaries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model blocks\n",
    "def conv_layer(input_tensor, kernel_shape, layer_name):\n",
    "    # input_tensor b01c\n",
    "    # kernel_shape 01-in-out\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "                               initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    biases = tf.get_variable(\"biases\", [kernel_shape[3]],\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    tf.histogram_summary(layer_name + \"/weights\", weights)\n",
    "    tf.histogram_summary(layer_name + \"/biases\", biases)\n",
    "    \n",
    "    # Other options are to use He et. al init. for weights and 0.01 \n",
    "    # to init. biases.\n",
    "    conv = tf.nn.conv2d(input_tensor, weights, \n",
    "                       strides = [1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def fc_layer(input_tensor, weights_shape, layer_name):\n",
    "    # weights_shape in-out\n",
    "    weights = tf.get_variable(\"weights\", weights_shape,\n",
    "                              initializer = tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(\"biases\", [weights_shape[1]],\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "    tf.histogram_summary(layer_name + \"/weights\", weights)\n",
    "    tf.histogram_summary(layer_name + \"/biases\", biases)\n",
    "    mult_out = tf.matmul(input_tensor, weights)\n",
    "    return tf.nn.relu(mult_out+biases)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "model_input = tf.placeholder(tf.float32, name='model_input')\n",
    "tf.image_summary('input', model_input, 10)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name='dropout_prob')\n",
    "\n",
    "target = tf.placeholder(tf.float32, name='target')\n",
    "\n",
    "layer_name = 'conv1'\n",
    "with tf.variable_scope(layer_name):\n",
    "    conv1_out = conv_layer(model_input, [5, 5, 3, 64], layer_name)\n",
    "\n",
    "pool1_out = tf.nn.max_pool(conv1_out, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME',\n",
    "                          name='pool1')\n",
    "\n",
    "layer_name = 'conv2'\n",
    "with tf.variable_scope(layer_name):\n",
    "    conv2_out = conv_layer(pool1_out, [5, 5, 64, 64], layer_name)\n",
    "    \n",
    "pool2_out = tf.nn.max_pool(conv2_out, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME',\n",
    "                          name='pool2')\n",
    "\n",
    "pool2_out_flat = tf.reshape(pool2_out, [-1, 8*8*64], name='pool2_flat')\n",
    "\n",
    "layer_name = 'fc1'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc1_out = fc_layer(pool2_out_flat, [8*8*64, 512], layer_name)\n",
    "\n",
    "fc1_out_drop = tf.nn.dropout(fc1_out, keep_prob)\n",
    "\n",
    "layer_name = 'fc2'\n",
    "with tf.variable_scope(layer_name):\n",
    "    fc2_out = fc_layer(fc1_out_drop, [512, 10], layer_name)\n",
    "    \n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(fc2_out, target,\n",
    "                                           name='cross_entropy'))\n",
    "tf.scalar_summary('cross_entropy', cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimization\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "grads_vars = optimizer.compute_gradients(cross_entropy)\n",
    "optimizer.apply_gradients(grads_vars)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "# Metrics\n",
    "correct_prediction = tf.equal(tf.argmax(fc2_out, 1),\n",
    "                             tf.argmax(target, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "tf.scalar_summary('accuracy', accuracy)\n",
    "\n",
    "# Useful training functions\n",
    "def validate():\n",
    "    data, labels = cifar10.getValidationSet()\n",
    "    acc = sess.run((accuracy),\n",
    "                   feed_dict={\n",
    "            model_input: data,\n",
    "            target: labels,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "    return acc\n",
    "def test():\n",
    "    data, labels = cifar10.getTestSet()\n",
    "    acc = sess.run((accuracy),\n",
    "                   feed_dict={\n",
    "            model_input: data,\n",
    "            target: labels,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "merged = tf.merge_all_summaries()\n",
    "train_writer = tf.train.SummaryWriter(SUMMARIES_DIR,\n",
    "                                     sess.graph)\n",
    "sess.run(tf.initialize_all_variables())\n",
    "cifar10.reset()\n",
    "print \"Trainable variables\"\n",
    "for n in tf.trainable_variables():\n",
    "    print n.name\n",
    "epochs = 3\n",
    "mean_gradients = np.zeros([len(tf.trainable_variables()),epochs])\n",
    "std_gradients = np.zeros([len(tf.trainable_variables()),epochs])\n",
    "\n",
    "t_i = time.time()\n",
    "n_batches = cifar10.n_batches\n",
    "while cifar10.getEpoch()<epochs:\n",
    "    epoch = cifar10.getEpoch()\n",
    "    batch, batch_idx = cifar10.nextBatch()\n",
    "    batch_data = batch[0]\n",
    "    batch_labels = batch[1]\n",
    "    summary, _, loss, grads = sess.run((merged, train_step, cross_entropy, grads_vars), \n",
    "                      feed_dict={\n",
    "            model_input: batch_data,\n",
    "            target: batch_labels,\n",
    "            keep_prob: 0.5\n",
    "        })\n",
    "\n",
    "    step = batch_idx+epoch*n_batches\n",
    "    if step%10==0:\n",
    "        train_writer.add_summary(summary, step)\n",
    "    if batch_idx==0:\n",
    "        for layer in range(len(tf.trainable_variables())):\n",
    "            mean_gradients[layer,epoch] = np.mean(np.abs(grads[layer][0]))\n",
    "            std_gradients[layer,epoch] = np.std(np.abs(grads[layer][0]))\n",
    "        print \"Epoch %d, loss %f\" %(epoch, loss)\n",
    "        validation_accuracy = validate()\n",
    "        print \"Validation accuracy %f\"%(validation_accuracy)\n",
    "        print \"Time elapsed\", (time.time()-t_i)/60.0, \"minutes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(epochs)\n",
    "plt.errorbar(x,mean_gradients[0,:],std_gradients[0,:])\n",
    "plt.hold(True)\n",
    "plt.errorbar(x,mean_gradients[2,:],std_gradients[2,:])\n",
    "plt.errorbar(x,mean_gradients[4,:],std_gradients[4,:])\n",
    "plt.errorbar(x,mean_gradients[6,:],std_gradients[6,:])\n",
    "plt.ylabel('Gradient')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Weights Gradient by Layer')\n",
    "plt.legend([\"conv1\", \"conv2\",\"fc1\",\"fc2\"])\n",
    "plt.hold(False)\n",
    "plt.show()\n",
    "plt.errorbar(x,mean_gradients[1,:],std_gradients[1,:])\n",
    "plt.hold(True)\n",
    "plt.errorbar(x,mean_gradients[3,:],std_gradients[3,:])\n",
    "plt.errorbar(x,mean_gradients[5,:],std_gradients[5,:])\n",
    "plt.errorbar(x,mean_gradients[7,:],std_gradients[7,:])\n",
    "plt.ylabel('Gradient')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Biases Gradient by Layer')\n",
    "plt.legend([\"conv1\", \"conv2\",\"fc1\",\"fc2\"])\n",
    "plt.hold(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
