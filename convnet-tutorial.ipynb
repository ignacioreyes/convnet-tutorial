{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some consideration before starting:\n",
    "-Edit cifar10.py file, setting the directory where CIFAR10 dataset is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from cifar10 import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "cifar10 = CIFAR10(batch_size=100, validation_proportion=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model blocks\n",
    "def conv_layer(input_tensor, kernel_shape):\n",
    "    # input_tensor b01c\n",
    "    # kernel_shape 01-in-out\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "                               initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    biases = tf.get_variable(\"biases\", [kernel_shape[3]],\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "    # Other options are to use He et. al init. for weights and 0.01 \n",
    "    # to init. biases.\n",
    "    conv = tf.nn.conv2d(input_tensor, weights, \n",
    "                       strides = [1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def fc_layer(input_tensor, weights_shape):\n",
    "    # weights_shape in-out\n",
    "    weights = tf.get_variable(\"weights\", weights_shape,\n",
    "                              initializer = tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(\"biases\", [weights_shape[1]],\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "    mult_out = tf.matmul(input_tensor, weights)\n",
    "    return tf.nn.relu(mult_out+biases)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "model_input = tf.placeholder(tf.float32, name='model_input')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name='dropout_prob')\n",
    "\n",
    "target = tf.placeholder(tf.float32, name='target')\n",
    "\n",
    "with tf.variable_scope('conv1'):\n",
    "    conv1_out = conv_layer(model_input, [5, 5, 3, 64])\n",
    "\n",
    "pool1_out = tf.nn.max_pool(conv1_out, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME',\n",
    "                          name='pool1')\n",
    "\n",
    "with tf.variable_scope('conv2'):\n",
    "    conv2_out = conv_layer(pool1_out, [5, 5, 64, 64])\n",
    "    \n",
    "pool2_out = tf.nn.max_pool(conv2_out, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME',\n",
    "                          name='pool2')\n",
    "\n",
    "pool2_out_flat = tf.reshape(pool2_out, [-1, 8*8*64], name='pool2_flat')\n",
    "with tf.variable_scope('fc1'):\n",
    "    fc1_out = fc_layer(pool2_out_flat, [8*8*64, 512])\n",
    "\n",
    "fc1_out_drop = tf.nn.dropout(fc1_out, keep_prob)\n",
    "\n",
    "with tf.variable_scope('fc2'):\n",
    "    fc2_out = fc_layer(fc1_out_drop, [512, 10])\n",
    "    \n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(fc2_out, target,\n",
    "                                           name='cross_entropy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimization\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# Metrics\n",
    "correct_prediction = tf.equal(tf.argmax(fc2_out, 1),\n",
    "                             tf.argmax(target, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Useful training functions\n",
    "def validate():\n",
    "    data, labels = cifar10.getValidationSet()\n",
    "    return accuracy.eval(feed_dict={\n",
    "            model_input: data,\n",
    "            target: labels,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "def test():\n",
    "    data, labels = cifar10.getTestSet()\n",
    "    return accuracy.eval(feed_dict={\n",
    "            model_input: data,\n",
    "            target: labels,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 2.303765\n",
      "Validation accuracy 0.107200\n",
      "Test_accuracy 0.106000\n",
      "Epoch 1, loss 1.513935\n",
      "Validation accuracy 0.255400\n",
      "Test_accuracy 0.525100\n",
      "Epoch 2, loss 1.314660\n",
      "Validation accuracy 0.311800\n",
      "Test_accuracy 0.579900\n",
      "Epoch 3, loss 1.145095\n",
      "Validation accuracy 0.385600\n",
      "Test_accuracy 0.616300\n",
      "Epoch 4, loss 1.099893\n",
      "Validation accuracy 0.380200\n",
      "Test_accuracy 0.644600\n",
      "Epoch 5, loss 0.946965\n",
      "Validation accuracy 0.428800\n",
      "Test_accuracy 0.664700\n",
      "Epoch 6, loss 0.912110\n",
      "Validation accuracy 0.433200\n",
      "Test_accuracy 0.679000\n",
      "Epoch 7, loss 0.830126\n",
      "Validation accuracy 0.432600\n",
      "Test_accuracy 0.687600\n",
      "Epoch 8, loss 0.783433\n",
      "Validation accuracy 0.427400\n",
      "Test_accuracy 0.698100\n",
      "Epoch 9, loss 0.723293\n",
      "Validation accuracy 0.462200\n",
      "Test_accuracy 0.704600\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "sess.run(tf.initialize_all_variables())\n",
    "cifar10.reset()\n",
    "\n",
    "n_batches = cifar10.n_batches\n",
    "while cifar10.getEpoch()<10:\n",
    "    epoch = cifar10.getEpoch()\n",
    "    batch, batch_idx = cifar10.nextBatch()\n",
    "    batch_data = batch[0]\n",
    "    batch_labels = batch[1]\n",
    "    _, loss = sess.run((train_step, cross_entropy), \n",
    "                      feed_dict={\n",
    "            model_input: batch_data,\n",
    "            target: batch_labels,\n",
    "            keep_prob: 0.5\n",
    "        })\n",
    "    if batch_idx==0:\n",
    "        print \"Epoch %d, loss %f\" %(epoch, loss)\n",
    "        validation_accuracy = validate()\n",
    "        print \"Validation accuracy %f\"%(validation_accuracy)\n",
    "        test_accuracy = test()\n",
    "        print \"Test_accuracy %f\"%(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
